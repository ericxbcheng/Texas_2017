---
title: "Texas Machine Learning 5: Classification for Fumonisin"
author: "Xianbin Cheng"
date: "12/19/2019"
output: html_document
---

# Objective

  * After first trials, we train better classification models with higher sensitivity
  
# Method

1. Read files and load libraries.

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(caret)
library(mda)
library(kernlab)
library(gbm)
library(ada)
library(ranger)
library(kableExtra)
library(doParallel)
```

```{r, message = FALSE, warning = FALSE}
norm_data = read_csv(file = "tx x_norm_conc_1680_obs.csv")
FM_class = norm_data$FM_class
FM_class[FM_class == "M"] = "H"
spec_data = norm_data %>%
  dplyr::select(-c(X1, Kernel_ID, AF_class, FM_class, Spec_ID_all)) %>%
  as.matrix()

str(norm_data, list.len = 8, give.attr = FALSE)
summary(as.factor(FM_class))
```

2. Train models

  * Train-test split ratio = 7:3
  * Up-sampling of the minority class in the training set
  * 5-fold cross-validation
  * Using "sensitivity" as a metric for parameter tuning
  * Algorithms:


```{r}
set.seed(123)
trn_idx = createDataPartition(y = FM_class, p = 0.7, list = FALSE)
trn_data = spec_data[trn_idx, ]
tst_data = spec_data[-trn_idx, ]
trn_FM = FM_class[trn_idx] %>% as.factor()
tst_FM = FM_class[-trn_idx] %>% as.factor()
summary(trn_FM)
summary(tst_FM)

# Up sampling of the minority class
resamp = upSample(x = trn_data, y = trn_FM)
trn_data_samp = dplyr::select(.data = resamp, -Class) %>% as.matrix()
trn_FM_samp = resamp$Class
summary(trn_FM_samp)
```

```{r, echo = FALSE}
rm(spec_data, norm_data, resamp, trn_data_samp)
```

```{r, eval = FALSE}
# TrainControl
my_tolerance = function(x, metric, tol = 5, maximize){
  tolerance(x = x, metric = metric, tol = tol, maximize = maximize)
}
cv_5 = trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary, 
                    selectionFunction = "my_tolerance", verboseIter = FALSE, allowParallel = TRUE)
wt = ifelse(test = trn_FM_samp == "H", yes = 10, no = 1)
```

```{r, eval = FALSE}
# Parallel computation
cl = makeCluster(detectCores() - 2)
registerDoParallel(cl)

# gradient stochastic boosting
set.seed(123)
mod_gbm = train(x = trn_data_samp, 
                y = trn_FM_samp, 
                method = "gbm", 
                preProcess = NULL, 
                metric = "Sens",
                trControl = cv_5, 
                verbose = FALSE,
                tuneGrid = expand.grid(shrinkage = c(0.01, 0.05, 0.1), 
                                       n.minobsinnode = 50, 
                                       n.trees = c(50, 100, 150), 
                                       interaction.depth = 1))
saveRDS(object = mod_gbm, file = "mod_gbm_fm_2.rds")

# SVM
set.seed(123)
mod_svm_rad = train(x = trn_data_samp, 
                    y = trn_FM_samp, 
                    method = "svmRadial", 
                    preProcess = NULL, 
                    metric = "Sens",
                    trControl = cv_5, 
                    tuneGrid = expand.grid(sigma = c(0.001, 0.01), 
                                           C = c(0.1, 0.3, 0.5)))
saveRDS(object = mod_svm_rad, file = "mod_svm_rad_fm_2.rds")

# PDA
set.seed(123)
mod_pda = train(x = trn_data_samp, 
                    y = trn_FM_samp, 
                    method = "pda2", 
                    preProcess = NULL, 
                    metric = "Sens",
                    trControl = cv_5, 
                    tuneGrid = expand.grid(df = c(10, 20, 30, 40, 50)))
saveRDS(object = mod_pda, file = "mod_pda_fm_2.rds")

# Ada
set.seed(123)
mod_ada = train(x = trn_data_samp, 
                    y = trn_FM_samp, 
                    method = "ada", 
                    preProcess = NULL, 
                    metric = "Sens",
                    trControl = cv_5, 
                    tuneGrid = expand.grid(iter = c(50, 100, 150), 
                                           maxdepth = 1, 
                                           nu = c(0.01, 0.05, 0.1)))
saveRDS(object = mod_ada, file = "mod_ada_fm_2.rds")

# ranger Rf
set.seed(123)
mod_rf = train(x = trn_data_samp,
               y = trn_FM_samp,
               method = "ranger",
               preProcess = NULL,
               metric = "Sens",
               trControl = cv_5,
               num.trees = 100,
               tuneGrid = expand.grid(min.node.size = c(600, 700, 800), 
                                      mtry = c(1, round(sqrt(ncol(trn_data_samp)))), 
                                      splitrule = "gini"))
saveRDS(object = mod_rf, file = "mod_rf_fm_2.rds")

stopCluster(cl)

##### nnet failed
# set.seed(123)
# mod_nnet = nnet(x = trn_data_samp, y = trn_FM_samp, size = 1, MaxNWts = 1600)

# QDA (collinear variables --> failed)
```

```{r, echo = FALSE}
rds_files = dir(pattern = "_2.rds")
mods = map(.x = rds_files, .f = readRDS)
names(mods) = str_remove(string = rds_files, pattern = "_2.rds")
```

```{r, echo = FALSE}
get_metrics = function(mod, true){
  
  a = confusionMatrix(data = as.factor(mod), reference = as.factor(true), positive = "H")
  accu = a$overall["Accuracy"]
  sens = a$byClass["Sensitivity"]
  spec = a$byClass["Specificity"]
  return(list(accu, sens, spec))
}

# Plot non-zero beta_hats
get_nonzero = function(model){
  
  a = model$beta %>%
    as.matrix() %>%
    as.data.frame() %>%
    subset(x = ., subset = `s0` != 0)
  
  wavelength = rownames(a) %>% 
    str_remove(string = ., pattern = "X") %>%
    as.numeric()
  
  a$wavelength = wavelength
  colnames(a)[1] = "beta"
  return(a)
}
```

```{r, echo = FALSE}
# GBM
result_gbm_trn = predict(object = mods$mod_gbm_fm, newdata = trn_data, type = "raw")
result_gbm = predict(object = mods$mod_gbm_fm, newdata = tst_data, type = "raw")

# SVM radial basis kernel
result_svm_rad_trn = predict(object = mods$mod_svm_rad_fm$finalModel, newdata = trn_data)
result_svm_rad = predict(object = mods$mod_svm_rad_fm$finalModel, newdata = tst_data)

# PDA
result_pda_trn = predict(object = mods$mod_pda_fm, newdata = trn_data)
result_pda = predict(object = mods$mod_pda_fm, newdata = tst_data)

# Ada
result_ada_trn = predict(object = mods$mod_ada_fm, newdata = trn_data)
result_ada = predict(object = mods$mod_ada_fm, newdata = tst_data)

# Rf (ranger)
result_rf_trn = predict(object = mods$mod_rf_fm, newdata = trn_data)
result_rf = predict(object = mods$mod_rf_fm, newdata = tst_data)
```

# Result

  Training and testing sensitivity and specificity values of each model with tuned parameters are shown here.

```{r, echo = FALSE}
list_trn = list(result_ada_trn, result_gbm_trn, result_pda_trn, result_rf_trn, result_svm_rad_trn)
list_tst = list(result_ada, result_gbm, result_pda, result_rf, result_svm_rad)

metrics_trn = sapply(X = list_trn, FUN = get_metrics, true = trn_FM)
metrics_tst = sapply(X = list_tst, FUN = get_metrics, true = tst_FM)

get_bestTune = function(model){
  a = model$bestTune
  param = colnames(a)
  return(str_c(param, a, sep = " = ", collapse = ", "))
}

result_table = rbind.data.frame(t(metrics_trn), t(metrics_tst)) %>%
  rename(.data = ., "Accuracy" = "V1", "Sensitivity" = "V2", "Specificity" = "V3") %>%
  mutate(Model = rep(x = c("Ada", "gbm", "PDA", "rf", "SVM_radial"), times = 2),
         Dataset = rep(x = c("training", "testing"), each = 5), 
         Parameters = rep(x = sapply(X = mods, FUN = get_bestTune), times = 2)) %>%
  dplyr::select(.data = ., c("Model", "Parameters","Dataset", "Accuracy", "Sensitivity", "Specificity")) %>%
  arrange(Model, desc(Dataset))

result_table$Accuracy = unlist(result_table$Accuracy)
result_table$Sensitivity = unlist(result_table$Sensitivity)
result_table$Specificity = unlist(result_table$Specificity)

kable_styling(kable_input = kable(result_table, format = "html", digits = 4), full_width = TRUE)
```

```{r, echo = FALSE}
temp = result_table %>%
  gather(data = ., key = "Metric", value = "Value", -c(Model, Dataset, Parameters))
temp$Value = unlist(temp$Value)

ggplot(data = temp, aes(x = as.factor(Model), y = Value, color = as.factor(Metric))) +
  geom_point() +
  scale_color_discrete(name = "Metric") +
  labs(x = "Model", "Metric Value") +
  coord_cartesian(ylim = c(0,1)) +
  facet_grid(as.factor(Dataset) ~ .) +
  theme_bw()
```

# Conclusion

* The **penalized discriminant analysis** model performs the best with a test sensitivity of 0.64 and a specificity of 0.89. 
* Models like GBM, Adaboost, random forest tend to overfit the training data, so for gbm and Adaboost models it would be better to reduce number of trees; for random forest, it would be better to increase the number of minimum node size and limit the `mtry`.

# Appendix

1. The details of parameter tuning of each model are presented here.

```{r, echo = FALSE}
mods$mod_gbm_fm
mods$mod_pda_fm
mods$mod_svm_rad_fm
mods$mod_ada_fm
mods$mod_rf_fm
```

2. Reproducibility

```{r, echo = FALSE}
sessionInfo()
```

